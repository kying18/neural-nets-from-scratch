# Neural Nets from Scratch

A simple, educational neural network library implemented from scratch using NumPy. This project demonstrates the fundamentals of deep learning by implementing core components like layers, activations, loss functions, and backpropagation.

## Features

### Models

- **Linear**: Fully connected layer
- **Sequential**: Container for chaining multiple layers together

### Activations

- **ReLU**: Rectified Linear Unit
- **Sigmoid**: Sigmoid activation function
- **Softmax**: Softmax activation for multi-class classification
- **Tanh**: Hyperbolic tangent activation

### Loss Functions

- **L1Loss**: Mean Absolute Error
- **L2Loss**: Mean Squared Error
- **BinaryCrossEntropyLoss**: Binary cross-entropy for binary classification
- **CategoricalCrossEntropyLoss**: Cross-entropy for multi-class classification

## Installation

TODO

## Usage

TODO

## Project Structure

```
src/nn_library/
├── models/          # Neural network layers
├── activation/      # Activation functions
└── loss/           # Loss functions
```

## Key Design Principles

- **Modular**: Each component (layer, activation, loss) is independent
- **Educational**: Clear implementation to understand how neural networks work
- **From Scratch**: Built using only NumPy, no deep learning frameworks
- **Backpropagation**: Full implementation of automatic differentiation

## License

This is an educational project for learning purposes. Feel free to use it. Please credit me!
